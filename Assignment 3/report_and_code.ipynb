{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70056f09",
   "metadata": {},
   "source": [
    "BBM409: Introduction to Machine Learning Lab.<br>\n",
    "Instructor: Ahmet Burak Can<br>\n",
    "TA: Burçak Asal\n",
    "\n",
    "# Assignment 3 : Naive Bayes Algorithm\n",
    "\n",
    "This assignment has done by **Desmin Alpaslan** (Student ID: 21945795) and **Mert Doğramacı** (Student ID: 21946055).\n",
    "\n",
    "### Contents\n",
    "**[Problem Definition and Data](#problem_definition)**<br>\n",
    "**[Part 1: Understanding the Data](#part1)**<br>\n",
    "**[Part 2: Implementing Naive Bayes](#part2)**<br>\n",
    "&emsp;2.1. [Implementation of Nested Subfunctions](#subfunctions)<br>\n",
    "&emsp;2.2. [Final Implementation of Naive Bayes](#naive_bayes)<br>\n",
    "**[Part 3: Analyzes](#part3)**<br>\n",
    "&emsp;3.a. [Analyzing effect of the words on prediction](#3a)<br>\n",
    "&emsp;&emsp;3.a.1. *[List the 10 words whose presence most strongly predicts that the mail is ham](#3a1)*<br>\n",
    "&emsp;&emsp;3.a.2. *[List the 10 words whose absence most strongly predicts that the mail is ham](#3a2)*<br>\n",
    "&emsp;&emsp;3.a.3. *[List the 10 words whose presence most strongly predicts that the mail is spam](#3a3)*<br>\n",
    "&emsp;&emsp;3.a.4. *[List the 10 words whose absence most strongly predicts that the mail is spam](#3a4)*<br>\n",
    "&emsp;3.b. [Stopwords](#3b)<br>\n",
    "&emsp;3.c. [Analyzing effect of the stopwords](#3c)<br>\n",
    "**[Part 4: Calculation of Performance Metrics](#part4)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e2c18",
   "metadata": {},
   "source": [
    "## 1. Problem Definition and Data<a class=\"anchor\" id=\"problem_definition\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a146e1",
   "metadata": {},
   "source": [
    "In this assignment, we will try to `determine whether a mail is ham or spam` from a given mail dataset. We will do it with the help of a Naive Bayes classifier `that we will implement` and verify its performance on again given E-Mail Spam Dataset. We will use the `Naive Bayes classifier algorithm`, that we learned in the class, during this assignment.\n",
    "\n",
    "As I stated before, a dataset is provided us for both training and validation phases named `emails.csv` which is avaible in the post of assignment 3 at Piazza page. We included dataset with the path as `\"emails.csv\"`. If you will change the dataset or its location, you should change the argument of the read_csv() function. \n",
    "\n",
    "You can see the whole dataset and its shape below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df9f2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1139d1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5723</th>\n",
       "      <td>Subject: re : research and development charges...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5724</th>\n",
       "      <td>Subject: re : receipts from visit  jim ,  than...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5725</th>\n",
       "      <td>Subject: re : enron case study update  wow ! a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5726</th>\n",
       "      <td>Subject: re : interest  david ,  please , call...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5727</th>\n",
       "      <td>Subject: news : aurora 5 . 2 update  aurora ve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5728 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  spam\n",
       "0     Subject: naturally irresistible your corporate...     1\n",
       "1     Subject: the stock trading gunslinger  fanny i...     1\n",
       "2     Subject: unbelievable new homes made easy  im ...     1\n",
       "3     Subject: 4 color printing special  request add...     1\n",
       "4     Subject: do not have money , get software cds ...     1\n",
       "...                                                 ...   ...\n",
       "5723  Subject: re : research and development charges...     0\n",
       "5724  Subject: re : receipts from visit  jim ,  than...     0\n",
       "5725  Subject: re : enron case study update  wow ! a...     0\n",
       "5726  Subject: re : interest  david ,  please , call...     0\n",
       "5727  Subject: news : aurora 5 . 2 update  aurora ve...     0\n",
       "\n",
       "[5728 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"emails.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1567ee",
   "metadata": {},
   "source": [
    "E-Mail Spam Dataset is a dataset that consists of 5728 samples with features as the text of mail and spam label. Spam label has two values 0 and 1.<br>\n",
    "0: Ham<br>1: Spam<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f9f10b",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Data<a class=\"anchor\" id=\"part1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fc1e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d93cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"emails.csv\").to_numpy()\n",
    "corpus = data[:,0]\n",
    "y = data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f23a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", ngram_range = (1,1)) # only consider unigrams\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "X = np.squeeze(np.asarray(X))\n",
    "N, M = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0312960f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total of 1368 spam and 4360 ham mails\n",
      "(1368, 37303) (4360, 37303)\n"
     ]
    }
   ],
   "source": [
    "freq_words = np.zeros((M, 2))\n",
    "spam_idx, ham_idx = y == 1, y == 0\n",
    "\n",
    "print(\"There are total of %d spam and %d ham mails\" % (X[spam_idx].shape[0], X[ham_idx].shape[0]))\n",
    "print(X[spam_idx].shape, X[ham_idx].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da146a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words only seen in spam mails: 10229\n",
      "Number of words only seen in ham mails: 18529\n"
     ]
    }
   ],
   "source": [
    "spam_num, ham_num = X[spam_idx].sum(axis=0), X[ham_idx].sum(axis=0)\n",
    "freq_words[:,0] += spam_num\n",
    "freq_words[:,1] += ham_num\n",
    "\n",
    "print(\"Number of words only seen in spam mails:\", np.sum(freq_words[:,1] == 0))\n",
    "print(\"Number of words only seen in ham mails:\", np.sum(freq_words[:,0] == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43ee43f",
   "metadata": {},
   "source": [
    "Some statistics about the data:\n",
    "1. There are total of **37303** distinct words in the dataset and **5728** lines of mails.\n",
    "2. In these words, **10229** of them is seen only in spam mails and **18529** of them is seen only in ham mails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbe53090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Words with highest R_s ####\n",
      "Word\tR_s\tN_s\tN\n",
      "projecthoneypot\t1.00\t110.0\t110\n",
      "viagra\t1.00\t174.0\t174\n",
      "stationery\t1.00\t120.0\t120\n",
      "2005\t0.99\t374.0\t379\n",
      "engines\t0.97\t112.0\t115\n",
      "advertisement\t0.97\t102.0\t105\n",
      "adobe\t0.97\t462.0\t476\n",
      "jul\t0.96\t162.0\t168\n",
      "2004\t0.95\t169.0\t177\n",
      "grants\t0.95\t110.0\t116\n",
      "#### Words with highest R_h ####\n",
      "Word\tR_h\tN_h\tN\n",
      "na\t0.99\t616.0\t623\n",
      "model\t0.99\t1287.0\t1306\n",
      "attached\t0.98\t898.0\t912\n",
      "schedule\t0.98\t637.0\t647\n",
      "option\t0.98\t561.0\t570\n",
      "london\t0.98\t828.0\t843\n",
      "09\t0.98\t1085.0\t1105\n",
      "john\t0.98\t1016.0\t1035\n",
      "summer\t0.98\t617.0\t629\n",
      "08\t0.98\t1192.0\t1216\n"
     ]
    }
   ],
   "source": [
    "ratios_s = freq_words[:,0] / X.sum(axis=0) # total times in spam / total usage of the word\n",
    "ratios_h = freq_words[:,1] / X.sum(axis=0) # total times in ham / total usage of the word\n",
    "idx_s_rats = np.argsort(ratios_s)[::-1]\n",
    "idx_h_rats = np.argsort(ratios_h)[::-1]\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "print(\"#### Words with highest R_s ####\")\n",
    "count = 0\n",
    "print(\"Word\",\"R_s\",\"N_s\",\"N\", sep=\"\\t\")\n",
    "for i in idx_s_rats:\n",
    "    if X[:,i].sum() > 100:       \n",
    "        print(words[i], \"%.2f\" % ratios_s[i], freq_words[i,0], X[:,i].sum(), sep='\\t')\n",
    "        count += 1\n",
    "        if count == 10:\n",
    "            break\n",
    "\n",
    "print(\"#### Words with highest R_h ####\")\n",
    "count = 0\n",
    "print(\"Word\",\"R_h\",\"N_h\",\"N\", sep=\"\\t\")\n",
    "for i in idx_h_rats:\n",
    "    if ratios_h[i] < 0.99 and X[:,i].sum() > 500:       \n",
    "        print(words[i], \"%.2f\" % ratios_h[i], freq_words[i,1], X[:,i].sum(), sep='\\t')\n",
    "        count += 1\n",
    "        if count == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed2e6ba",
   "metadata": {},
   "source": [
    "We compare the words according to their spam ratio which is defined as follows:<br>\n",
    "<br>\n",
    "$$\\large R_s = \\dfrac{N_s}{N},\\ R_h = \\dfrac{N_h}{N} $$<br>\n",
    "where:\n",
    "- $N_s$ number of occurances in a spam mail of the word.\n",
    "- $N_h$ number of occurances in a ham mail of the word\n",
    "- $N$ is the total occurances.<br>\n",
    "\n",
    "In the upper cell, we print the 10 words with highest $R_s$ and $N > 100$, highest $R_h$ and $N > 500$. We selected the 3 words among them and inspect their statistics:\n",
    "1. **viagra**: We see that in this dataset all the mails that includes \"viagra\" are **spam**, since $R_s = 1.0$. Even though the $N$ is quite small (174), from prior experience we know that these type of mails are usually spam.\n",
    "2. **adobe**: We see that in this dataset most of the mails that includes \"adobe\" are spam, with $R_s = 0.97$. Furthermore because $N = 476$ and $N_s = 462$ (which are quite high occurances), we can conclude that this word provides a useful distinction between two type of mails.\n",
    "3. **schedule**: We see that in this dataset mos of the mails that includes \"schedule\" are ham, with $R_h = 0.98$. We know that from prior experience that mails that mentions schedule are usually not spam.\n",
    "\n",
    "We can conclude that it is feasible to label mails as spam or ham by looking at the words. However there are few drawbacks in this dataset:\n",
    "1. Even though some of the words has high $R_s$ their $N$ is quite low (< 100). This will result in a **biased prediction**.\n",
    "2. There are some words that are only numbers (09, 08, 2005, 2004) which shouldn't be telling a much about the type of the mail. However because of the dataset, some of these words has high $R_s$ and $R_h$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe1215",
   "metadata": {},
   "source": [
    "## Part 2: Implementing Naive Bayes<a class=\"anchor\" id=\"part2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ba63d4",
   "metadata": {},
   "source": [
    "For being able to calculate the probability of being a spam or ham mail of an unknown mail sample, we should solve the below equation:\n",
    "\n",
    "$$\\large P(y = spam|word) = \\dfrac{P(word|y = spam) * P(y = spam)}{P(word)}$$<br>\n",
    "$$\\large P(y = ham|word) = \\dfrac{P(word|y = ham) * P(y = ham)}{P(word)}$$<br>\n",
    "\n",
    "As you can see, the denominators of the both equation are same. Therefore, we don't need to $P(word)$ probabilities. \n",
    "\n",
    "In conclusion, we have to determine below equation for finding the label of the unknown mail example.\n",
    "\n",
    "$$\\large \\hat{y} = \\underset{y \\in spam, ham}{\\mathrm{argmax}} P(y|word) = P(word|y) * P(y)$$<br>\n",
    "\n",
    "For determining this value, we need to calculate $P(word|y = spam)$, $P(word|y = ham)$, $P(y = spam)$ and $P(y = ham)$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adb01cd",
   "metadata": {},
   "source": [
    "### 2.1. Implementation of Nested Subfunctions<a class=\"anchor\" id=\"subfunctions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de3302",
   "metadata": {},
   "source": [
    "**__vectorizer** is a helper function that we use for detecting every word in every mail sample and their counts for each mail. It means it creates a matrix which stores the words that emails includes and their counts in each email. The result matrix includes N (number of mails) number of rows and column number equals to number of unique words in mails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2337e309",
   "metadata": {},
   "source": [
    "```python  \n",
    "    def __vectorizer(self, arr: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Creates a matrix which stores the words that emails includes and their counts in each email\n",
    "\n",
    "        :param arr: an numpy array with shape (N,1) for\n",
    "        :return: a CountVectorizer matrix (N, number of different words in emails) and a vector named columns\n",
    "        with shape (number of different words in emails, 1) which stores all words appears in mails\n",
    "        \"\"\"\n",
    "\n",
    "        # initializes CountVectorizer item with ngram_mode\n",
    "        vectorizer = CountVectorizer(ngram_range=**Selected ngram mode will come here**, stop_words=self.stop_words)\n",
    "\n",
    "        # vector that holds all words in emails and their counts for each item\n",
    "        vector = vectorizer.fit_transform(arr)\n",
    "\n",
    "        # convert vector variable to array for usability\n",
    "        count_vector = vector.toarray()\n",
    "\n",
    "        # names of columns\n",
    "        columns = vectorizer.get_feature_names_out()\n",
    "\n",
    "        return count_vector, columns\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eef031",
   "metadata": {},
   "source": [
    "**__calculate_class_prior** function calculates following probability values (prior probabilities) as follows:<br><br>\n",
    "$$\\large P(y = spam) = \\dfrac{N_s}{N}$$<br>\n",
    "$$\\large P(y = ham) = \\dfrac{N_h}{N}$$<br>\n",
    "where:\n",
    "- $N_s$ number of spam mails\n",
    "- $N_h$ number of ham mails\n",
    "- $N$ the total number of mails<br>\n",
    "\n",
    "and stores them in a variable named probability_dict which is a class variable.\n",
    "\n",
    "We have used logarithm to prevent numerical underflow when calculating multiplicative probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6dbac2",
   "metadata": {},
   "source": [
    "```python\n",
    "    def __calculate_class_prior(self, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Calculates class probabilities [P(spam) and P(ham)] for training examples, and adds the result into\n",
    "        self.probability dictionary as \"spam\" and \"ham\" labels\n",
    "\n",
    "        :param y: data with shape (N,), each row consists of the label of the mail (0 for ham, 1 for spam)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        # labels are only 0 and 1 therefore if we sum all items we get number of 1s\n",
    "        # instead of a for loop we can use this method\n",
    "        number_of_spam = np.sum(y)\n",
    "        number_of_ham = len(y) - number_of_spam\n",
    "\n",
    "        self.probability_dict[\"spam\"] = np.log(number_of_spam / y.shape[0])  # P(spam) = number of spams / N\n",
    "        self.probability_dict[\"ham\"] = np.log(number_of_ham / y.shape[0])     # P(ham) = number of hams / N\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2eb2dc",
   "metadata": {},
   "source": [
    "**__calculate_likelihoods** function calculates the likelihoods probabilities as follows:<br><br>\n",
    "$$\\large P(word|y = spam) = \\dfrac{N_{ws}}{N_s}$$<br>\n",
    "$$\\large P(word|y = ham) = \\dfrac{N_{wh}}{N_h}$$<br>\n",
    "where:\n",
    "- $N_{ws}$ number of occurances of the word in all spam mails \n",
    "- $N_{wh}$ number of occurances of the word in all ham mails \n",
    "- $N_s$ total number of all word occurrences in all spam mails\n",
    "- $N_h$ total number of all word occurrences in all ham mails<br>\n",
    "\n",
    "and stores them in a variable named probability_dict which is a class variable.\n",
    "\n",
    "**Please not that! :** We take the logarithm of the probabilities to prevent numerical underflow when calculating multiplicative probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe65c627",
   "metadata": {},
   "source": [
    "```python\n",
    "    def __calculate_likelihoods(self, y: np.ndarray, columns: np.ndarray, alpha: int) -> None:\n",
    "        \"\"\"\n",
    "        Calculates likelihoods of each word that contains in emails as P(word|spam) and P(word|ham), and adds the \n",
    "        results into self.probability dictionary as \"word|spam\" and \"word|ham\" labels\n",
    "\n",
    "        :param y: data with shape (N,), each row consists of the label of the mail (0 for ham, 1 for spam)\n",
    "        :param columns: a vector with shape (number of different words in emails,1) which stores all words appears \n",
    "        in mails\n",
    "        :param alpha: int value for smoothing\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        N, D = self.count_vector.shape\n",
    "        spam_vector, ham_vector = np.sum(self.count_vector[y == 1], axis=0), np.sum(self.count_vector[y == 0], axis=0)\n",
    "\n",
    "        n_s = np.sum(spam_vector) # total number of all word occurrences in all spam mails\n",
    "        n_h = np.sum(ham_vector) # total number of all word occurrences in all ham mails\n",
    "\n",
    "        for word_i in range(D):\n",
    "            n_w_s = spam_vector[word_i] # number of occurances in a spam mail of the word\n",
    "            n_h_s = ham_vector[word_i] # number of occurances in a ham mail of the word\n",
    "\n",
    "            self.probability_dict[\"%s|spam\" % columns[word_i]] = np.log((n_w_s + alpha) / (n_s + D))\n",
    "            self.probability_dict[\"%s|ham\" % columns[word_i]] = np.log((n_h_s + alpha) / (n_h + D))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b2ed97",
   "metadata": {},
   "source": [
    "### 2.2. Final Implementation of Naive Bayes<a class=\"anchor\" id=\"naive_bayes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4619157",
   "metadata": {},
   "source": [
    "If we combine these subfunctions with main fit and predict functions, we get below algorithm and code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ca35263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from typing import Tuple\n",
    "\n",
    "MODES = {\n",
    "    \"unigram\": (1, 1),\n",
    "    \"bigram\": (2, 2)\n",
    "}\n",
    "\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, mode: str = \"unigram\", stop_words=None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize NaiveBayes model\n",
    "\n",
    "        Args:\n",
    "            mode (str, optional): Mode for BagOfWords, should be either Unigram or Bigram. Defaults to \"unigram\".\n",
    "            stop_words (list, optional): Stop words to eliminate from BagOfWords\n",
    "        \"\"\"\n",
    "        self.count_vector = None\n",
    "        self.probability_dict = dict()\n",
    "\n",
    "        assert mode in MODES.keys(), \"Mode should be either bigram or unigram\"\n",
    "        self.ngram_mode = MODES[mode]\n",
    "        self.stop_words = stop_words\n",
    "\n",
    "    def fit(self, x: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Fit the data, calculate probabilities according to it\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): data with shape (N,1), each row consists of a mail\n",
    "            y (np.ndarray): data with shape (N,), each row consists of the label of the mail (0 for ham, 1 for spam)\n",
    "        \"\"\"\n",
    "\n",
    "        # columns is list of every words without their counts, counts stores in count_vector\n",
    "        self.count_vector, columns = self.__vectorizer(x)\n",
    "\n",
    "        self.__calculate_class_prior(y)     # calculates P(spam) and P (ham), and adds them into probability_dict\n",
    "        # calculates P(x(i)|spam) and P(x(i)|ham) values, and adds them into probability_dict\n",
    "        self.__calculate_likelihoods(y, columns, 1)\n",
    "\n",
    "    def predict(self, x_predict: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the labels of the mails in x_predict\n",
    "\n",
    "        Args:\n",
    "            x_predict (np.ndarray): Data to be predicted, shape (N,1)\n",
    "\n",
    "        Returns:\n",
    "            y_predict (np.ndarray): Predictions for the mails, shape (N,)\n",
    "        \"\"\"\n",
    "\n",
    "        n = x_predict.shape[0]\n",
    "        y_predict = np.zeros(n)\n",
    "\n",
    "        vector, columns = self.__vectorizer(x_predict)\n",
    "\n",
    "        for i in range(n):\n",
    "            probability_of_spam = 0\n",
    "            probability_of_ham = 0\n",
    "\n",
    "            word_idx = np.arange(len(columns))[vector[i] > 0] # only work on words that the text has\n",
    "            # calculate P(vj | text)\n",
    "            for j in word_idx:\n",
    "                if \"%s|spam\" % columns[j] in self.probability_dict.keys():\n",
    "                    probability_of_spam += vector[i][j] * self.probability_dict[columns[j] + \"|spam\"]\n",
    "                    probability_of_ham += vector[i][j] * self.probability_dict[columns[j] + \"|ham\"]\n",
    "\n",
    "            probability_of_spam += self.probability_dict[\"spam\"]\n",
    "            probability_of_ham += self.probability_dict[\"ham\"]\n",
    "\n",
    "            y_predict[i] = 1 if probability_of_spam > probability_of_ham else 0\n",
    "\n",
    "        return y_predict\n",
    "\n",
    "    def __vectorizer(self, arr: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Creates a matrix which stores the words that emails includes and their counts in each email\n",
    "\n",
    "        :param arr: an numpy array with shape (N,1) for\n",
    "        :return: a CountVectorizer matrix (N, number of different words in emails) and a vector named columns\n",
    "        with shape (number of different words in emails, 1) which stores all words appears in mails\n",
    "        \"\"\"\n",
    "\n",
    "        # initializes CountVectorizer item with ngram_mode\n",
    "        vectorizer = CountVectorizer(ngram_range=self.ngram_mode, stop_words=self.stop_words)\n",
    "\n",
    "        # vector that holds all words in emails and their counts for each item\n",
    "        vector = vectorizer.fit_transform(arr)\n",
    "\n",
    "        # convert vector variable to array for usability\n",
    "        count_vector = vector.toarray()\n",
    "\n",
    "        # names of columns\n",
    "        columns = vectorizer.get_feature_names()\n",
    "\n",
    "        return count_vector, columns\n",
    "\n",
    "    def __calculate_class_prior(self, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Calculates class probabilities [P(spam) and P(ham)] for training examples, and adds the result into\n",
    "        self.probability dictionary as \"spam\" and \"ham\" labels\n",
    "\n",
    "        :param y: data with shape (N,), each row consists of the label of the mail (0 for ham, 1 for spam)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        # labels are only 0 and 1 therefore if we sum all items we get number of 1s\n",
    "        # instead of a for loop we can use this method\n",
    "        number_of_spam = np.sum(y)\n",
    "        number_of_ham = len(y) - number_of_spam\n",
    "\n",
    "        self.probability_dict[\"spam\"] = np.log(number_of_spam / y.shape[0])  # P(spam) = number of spams / N\n",
    "        self.probability_dict[\"ham\"] = np.log(number_of_ham / y.shape[0])     # P(ham) = number of hams / N\n",
    "\n",
    "    def __calculate_likelihoods(self, y: np.ndarray, columns: np.ndarray, alpha: int) -> None:\n",
    "        \"\"\"\n",
    "        Calculates likelihoods of each word that contains in emails as P(word|spam) and P(word|ham), and adds the results\n",
    "        into self.probability dictionary as \"word|spam\" and \"word|ham\" labels\n",
    "\n",
    "        :param y: data with shape (N,), each row consists of the label of the mail (0 for ham, 1 for spam)\n",
    "        :param columns: a vector with shape (number of different words in emails, 1) which stores all words appears in mails\n",
    "        :param alpha: int value for smoothing\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        N, D = self.count_vector.shape\n",
    "        spam_vector, ham_vector = np.sum(self.count_vector[y == 1], axis=0), np.sum(self.count_vector[y == 0], axis=0)\n",
    "\n",
    "        n_s = np.sum(spam_vector) # | Text_spam |\n",
    "        n_h = np.sum(ham_vector) # | Text_ham  |\n",
    "\n",
    "        for word_i in range(D):\n",
    "            n_w_s = spam_vector[word_i]\n",
    "            n_h_s = ham_vector[word_i]\n",
    "\n",
    "            self.probability_dict[\"%s|spam\" % columns[word_i]] = np.log((n_w_s + alpha) / (n_s + D))\n",
    "            self.probability_dict[\"%s|ham\" % columns[word_i]] = np.log((n_h_s + alpha) / (n_h + D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd1ea1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with ngram=unigram, stop_words=True:\t0.996510\n",
      "Accuracy with ngram=bigram, stop_words=True:\t0.988656\n",
      "Accuracy with ngram=unigram, stop_words=False:\t0.991274\n",
      "Accuracy with ngram=bigram, stop_words=False:\t0.989529\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"emails.csv\")\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "data = df.to_numpy()\n",
    "x, y = data[:, 0], data[:, -1]\n",
    "\n",
    "shuffled_x, shuffled_y = shuffle(x, y, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "args = [(\"unigram\", ENGLISH_STOP_WORDS), (\"bigram\", ENGLISH_STOP_WORDS), (\"unigram\", None), (\"bigram\", None)]\n",
    "for arg in args:\n",
    "    model = NaiveBayes(*arg)        # initializes the model\n",
    "    model.fit(X_train, y_train)      # training\n",
    "    y_predict = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test.astype(bool), y_predict.astype(bool))\n",
    "    print(\"Accuracy with ngram=%s, stop_words=%s:\\t%f\" % (arg[0], arg[1] != None, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf0eae",
   "metadata": {},
   "source": [
    "## Part 3: Analyzes<a class=\"anchor\" id=\"part3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3b1a09",
   "metadata": {},
   "source": [
    "### 3.a. Analyzing effect of the words on prediction<a class=\"anchor\" id=\"3a\"></a>\n",
    "\n",
    "To analyze the effects of the words on prediction we created a pipeline that works as follows:\n",
    "1. We first eliminate the words in the word set according to their tfidf values for each document class (spam, ham). This step is composed of few steps.\n",
    "    1. Compute tfidf values by using `TfidfTransformer`, we got a matrix of shape (N, D) (where N is equal to sample size and D equal to distinct word count). However we want an output that acts as a scoring metric among words so that we can eliminate among them.\n",
    "    2. Take sum among `axis = 0`, sum by column, this way our matrix is reduced to shape (D,).\n",
    "    3. Take the maximum 100 and minimum 100 tfidf valued words. These 200 words will the candidates for finding the words that suggests certain class by their presence/absence. To be accurate, we used minimum 100 tfidf values to be absence candidates and maximum 100 words to be presence candidates.\n",
    "2. At this step, we see that our saved matrices **contains some stop words**.  To analyze the effects of presence and absence of a word, we compute their posterior probability as follows:\n",
    "$$P(v_d|w) = \\frac{P(w|v_d) * P(v_d)}{P(w)} \\text{ where $P(v_d|w)$ is the probability that the class of the document is $v_d$ given that it contains $w$}$$\n",
    "<center>and\n",
    "$$P(v_d|\\neg w) = \\frac{P(w|v_d) * P(v_d)}{P(\\neg w)} \\text{ where $P(v_d|\\neg w)$ is the probability that the class of the document is $v_d$ given that it doesn't contains $w$}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84ad60",
   "metadata": {},
   "source": [
    "From our $\\text{N$\\dot{a}$ive Bayes}$ model, we know the conditional probilities $P(w|v_d)$, $P(\\neg w|v_d) = 1 - P(w|v_d)$ and prior probilities $P(\\text{spam})$, $P(\\text{ham})$. Hence, we only need to calculate $P(w)$ and $P(\\neg w)$. To calculate those we count the total occurance and divide it by total word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b12060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "\n",
    "def find_presence_absence_probilities (X : np.ndarray, y : np.ndarray, stop_words = None):\n",
    "    ## Create pipelines for two different document class\n",
    "    pipe_spam = Pipeline([('count', CountVectorizer(stop_words = stop_words)),\n",
    "                               ('tfidf', TfidfTransformer(use_idf = True, smooth_idf = True))]).fit(X[y==1])\n",
    "    pipe_ham = Pipeline([('count', CountVectorizer(stop_words = stop_words)),\n",
    "                               ('tfidf', TfidfTransformer(use_idf = True, smooth_idf = True))]).fit(X[y==0])\n",
    "    ## Step 1. Eliminate words that has low tfidf values\n",
    "\n",
    "    y_vals = [(1, pipe_spam), (0, pipe_ham)]\n",
    "    y_words = dict()\n",
    "    for y_val, pipe in y_vals:\n",
    "        tfidf_vals = pipe.transform(X[y==y_val])\n",
    "        words = pipe[\"count\"].get_feature_names_out()\n",
    "        sums = np.sum(tfidf_vals, axis=0).A1\n",
    "        idx = np.argsort(sums)\n",
    "\n",
    "        print(\"##### Highest TFIDF values for class %s #####\" % y_val)\n",
    "        print(words[idx[-100:]])\n",
    "        y_words[\"y = %d\" % y_val] = (idx[:100], idx[-100:])\n",
    "    \n",
    "    ## Step 2. Eliminate words that has low tfidf values\n",
    "    vectorizer = CountVectorizer()\n",
    "    counts = vectorizer.fit_transform(X)\n",
    "    presence_counts = counts.sum(axis=0).A1\n",
    "    total = counts.sum()\n",
    "\n",
    "    doc_classes = [(\"spam\", pipe_spam),\n",
    "                   (\"ham\", pipe_ham)]\n",
    "    word_p_a_dict = dict()\n",
    "    N = X.shape[0]\n",
    "    for doc_class, pipe in tqdm(doc_classes, desc=\"Calculating probilities for P(class | word in doc) and P(class | word not in doc)\"):\n",
    "        word_p_a_dict[\"%s absence\" % doc_class] = []\n",
    "        word_p_a_dict[\"%s presence\" % doc_class] = []\n",
    "        label = 1 if doc_class == \"spam\" else 0\n",
    "\n",
    "        low_tfidf, high_tfidf = y_words[\"y = %d\" % label]\n",
    "        class_prior = model.probability_dict[doc_class]\n",
    "        words = pipe[\"count\"].get_feature_names_out()\n",
    "\n",
    "        for word_idx in tqdm(high_tfidf, leave = False, desc=\"Doing presence %s\" % doc_class):\n",
    "            word = words[word_idx]\n",
    "            word_idx_count = np.where(vectorizer.get_feature_names_out() == word)[0][0]\n",
    "            cond_prob = model.probability_dict[\"%s|%s\" % (word, doc_class)]\n",
    "            presence_count = presence_counts[word_idx_count]\n",
    "            p1 = (cond_prob + class_prior) - np.log(presence_count / total)\n",
    "            word_p_a_dict[\"%s presence\" % doc_class].append((words[word_idx], np.exp(p1)))\n",
    "\n",
    "        for word_idx in tqdm(low_tfidf, leave = False, desc=\"Doing absence %s\" % doc_class):\n",
    "            word = words[word_idx]\n",
    "            word_idx_count = np.where(vectorizer.get_feature_names_out() == word)[0][0]\n",
    "\n",
    "            cond_prob = np.log(1 - np.exp(model.probability_dict[\"%s|%s\" % (word, doc_class)]))\n",
    "            presence_count = presence_counts[word_idx_count]\n",
    "            p1 = cond_prob + class_prior - np.log((total-presence_count) / total)\n",
    "            word_p_a_dict[\"%s absence\" % doc_class].append((words[word_idx], np.exp(p1)))\n",
    "    \n",
    "    return word_p_a_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cb031af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Highest TFIDF values for class 1 #####\n",
      "['visit' 'interested' 'how' 'marketing' 'way' 'may' 'was' 'home' 'net'\n",
      " 'about' 'start' '2005' 'receive' 'offer' '10' 'offers' '000' 'time' 'has'\n",
      " 'search' 'only' 'account' 'want' 'but' 'list' 'life' 'what' 'into'\n",
      " 'viagra' 'see' 'new' 'us' 'any' 'like' 'over' 'www' 'site' 'message'\n",
      " 'out' 'make' 'logo' '95' 'information' 'mail' 'one' 'online' 'best'\n",
      " 'please' 'need' 'more' 'save' 'free' 'my' 'no' 'do' 'money' 'now' 'an'\n",
      " 'just' 'company' 'http' 'get' 'email' 'can' 'if' 'all' 'adobe' 'at' 'as'\n",
      " 'click' 'by' 'website' 'business' 'software' 'com' 'here' 'on' 'will'\n",
      " 'or' 'not' 'have' 'are' 'that' 'with' 'from' 'subject' 'be' 'our' 'it'\n",
      " 'we' 'this' 'for' 'is' 'in' 'of' 'your' 'and' 'you' 'the' 'to']\n",
      "##### Highest TFIDF values for class 0 #####\n",
      "['regards' 'him' 'his' 'email' 'forward' '713' 'but' 'up' 'call' 'resume'\n",
      " 'corp' 'some' 'information' 'need' 'get' 'crenshaw' '04' 'so' 'power'\n",
      " '12' 'very' 'conference' 'request' 'stinson' 'th' '11' 'model' 'do'\n",
      " 'houston' '01' 'new' 'all' '30' 'has' 'may' 'was' 'interview' 'about'\n",
      " 'energy' 'like' 'edu' 'let' '00' 'any' 'risk' '10' 'time' 'meeting' 'an'\n",
      " 'know' 'group' 'he' 'shirley' 'not' 'our' 'thanks' 'cc' 'can' 'or' '2001'\n",
      " 'pm' 'by' 'research' 'my' 're' 'if' 'are' 'would' 'as' 'from' 'me'\n",
      " 'please' 'com' 'it' 'am' 'kaminski' '2000' 'at' 'have' 'hou' 'your'\n",
      " 'will' 'with' 'that' 'this' 'be' 'we' 'subject' 'is' 'on' 'vince' 'in'\n",
      " 'for' 'enron' 'of' 'you' 'ect' 'and' 'to' 'the']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1926c30b7cc94997909fa2ebf7b7cfaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating probilities for P(class | word in doc) and P(class | word not in doc):   0%|          | 0/2 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doing presence spam:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doing absence spam:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doing presence ham:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doing absence ham:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_p_a_dict = find_presence_absence_probilities(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd6afd",
   "metadata": {},
   "source": [
    "From the outputs of above cell, we note that highest tfidf values contains some of the stop words such as `is`, `on`, `be`, `of` etc. In below cells, we calculate the posterior probilities similar to how we did in our $\\text{N$\\dot{a}$ive Bayes}$ model. However, since we saved our prior probilities and conditional probilities in log form we continue to do our calculations in log form. Below we show the formulation:\n",
    "$$ln{P(v_d|w)} = ln{\\frac{P(w|v_d) * P(v_d)}{P(w)}} \\tag{1}$$\n",
    "$$ln{P(v_d|w)} = ln{P(w|v_d)} + ln{P(v_d)} - ln{P(w)} \\tag{2}$$\n",
    "<br>\n",
    "<center>and\n",
    "<br>\n",
    "$$ln{P(v_d|\\neg w)} = ln{\\frac{P(\\neg w|v_d) * P(v_d)}{P(\\neg w)}} \\tag{3}$$\n",
    "$$ln{P(v_d|\\neg w)} = ln{P(\\neg w|v_d)} + ln{P(v_d)} - ln{P(\\neg w)} \\tag{4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9766de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "take_n = 10 # take n words with highest posterior probility\n",
    "for key, value in word_p_a_dict.items():\n",
    "    word_p_a_dict[key].sort(key= lambda x : x[1])\n",
    "    word_p_a_dict[key] = value[-take_n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895edbe0",
   "metadata": {},
   "source": [
    "#### 3.a.1. List the 10 words whose presence most strongly predicts that the mail is ham<a class=\"anchor\" id=\"3a1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94891f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\tProbability\n",
      "---------------------\n",
      "cc\t0.920928\n",
      "hou\t0.921190\n",
      "shirley\t0.921799\n",
      "ect\t0.922348\n",
      "vince\t0.922348\n",
      "enron\t0.922417\n",
      "kaminski\t0.922542\n",
      "crenshaw\t0.923175\n",
      "713\t0.923189\n",
      "stinson\t0.923226\n"
     ]
    }
   ],
   "source": [
    "query = \"ham presence\"\n",
    "print(\"Word\\tProbability\\n---------------------\")\n",
    "for word, prob in word_p_a_dict[query]:\n",
    "    print(\"%s\\t%f\" % (word, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119a7dc",
   "metadata": {},
   "source": [
    "#### 3.a.2. List the 10 words whose absence most strongly predicts that the mail is ham<a class=\"anchor\" id=\"3a2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4680fe13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\tProbability\n",
      "---------------------\n",
      "501\t0.761173\n",
      "unstable\t0.761173\n",
      "discharge\t0.761173\n",
      "interference\t0.761174\n",
      "renovation\t0.761174\n",
      "persistence\t0.761174\n",
      "cloak\t0.761174\n",
      "suffering\t0.761174\n",
      "secrecy\t0.761176\n",
      "php\t0.761206\n"
     ]
    }
   ],
   "source": [
    "query = \"ham absence\"\n",
    "print(\"Word\\tProbability\\n---------------------\")\n",
    "for word, prob in word_p_a_dict[query]:\n",
    "    print(\"%s\\t%f\" % (word, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b329d89d",
   "metadata": {},
   "source": [
    "#### 3.a.3. List the 10 words whose presence most strongly predicts that the mail is spam<a class=\"anchor\" id=\"3a3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b60f6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\tProbability\n",
      "---------------------\n",
      "click\t0.750847\n",
      "search\t0.783951\n",
      "life\t0.827946\n",
      "95\t0.843815\n",
      "save\t0.860793\n",
      "money\t0.882610\n",
      "logo\t0.945333\n",
      "adobe\t1.003532\n",
      "2005\t1.020820\n",
      "viagra\t1.037638\n"
     ]
    }
   ],
   "source": [
    "query = \"spam presence\"\n",
    "print(\"Word\\tProbability\\n---------------------\")\n",
    "for word, prob in word_p_a_dict[query]:\n",
    "    print(\"%s\\t%f\" % (word, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2168b",
   "metadata": {},
   "source": [
    "#### 3.a.4. List the 10 words whose absence most strongly predicts that the mail is spam<a class=\"anchor\" id=\"3a4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecc55394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\tProbability\n",
      "---------------------\n",
      "respected\t0.238828\n",
      "trash\t0.238829\n",
      "paige\t0.238829\n",
      "determining\t0.238829\n",
      "recovery\t0.238831\n",
      "sets\t0.238833\n",
      "sheet\t0.238835\n",
      "participating\t0.238837\n",
      "sometime\t0.238841\n",
      "steven\t0.238868\n"
     ]
    }
   ],
   "source": [
    "query = \"spam absence\"\n",
    "print(\"Word\\tProbability\\n---------------------\")\n",
    "for word, prob in word_p_a_dict[query]:\n",
    "    print(\"%s\\t%f\" % (word, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f36af9",
   "metadata": {},
   "source": [
    "We see that usually presence of certain words nearly gurantees that the document belongs to certain class. Such as `viagra`, where it's presence strongly suggests that the mail is spam. Similarly, absence of some words suggests to some level that it belongs to certain class. `secrecy` can be a great example for this, where it's absence strongly suggests that the mail is ham.<br>\n",
    "<br>\n",
    "Below we reimplemented our $\\text{N$\\dot{a}$ive Bayes}$ model to use tfidf values to discard not important words. Our implementation discards not important words based on their normalized tfidf values. Where we calculated the normalized tfidf values as follows:\n",
    "1. Sum by axis 0 to get a matrix of shape (C,), where C is the distinct word count\n",
    "2. Divide each column sum by document frequency (df), where df is defined as follows:\n",
    "    $$ df(D, w) = |\\{d \\in D, w \\in d\\}| \\text{, number of documents where word w appears} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4da70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "MODES = {\n",
    "    \"unigram\" : (1, 1),\n",
    "    \"bigram\" : (2, 2),\n",
    "    \"uni-bigram\" : (1, 2)\n",
    "}\n",
    "\n",
    "\n",
    "class NaiveBayesV2:\n",
    "    def __init__(self, mode: str = \"unigram\", stop_words = None, use_tfidf : bool = False, tfidf_th : float = None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize NaiveBayes model\n",
    "\n",
    "        Args:\n",
    "            mode (str, optional): Mode for BagOfWords, should be either Unigram or Bigram. Defaults to \"unigram\".\n",
    "            stop_words (list, optional): Stop words to eliminate from BagOfWords\n",
    "            use_idf (bool, optional)\n",
    "            tfidf_th (float, optional): Threshold for tfidf values, necessary only if use_tfidf is True\n",
    "        \"\"\"\n",
    "        self.count_vector = None\n",
    "        self.probability_dict = dict()\n",
    "\n",
    "        assert mode in MODES.keys(), \"Mode should be either bigram or unigram\"\n",
    "        self.ngram_mode = MODES[mode]\n",
    "        self.stop_words = stop_words\n",
    "        self.use_tfidf = use_tfidf\n",
    "        if use_tfidf:\n",
    "            self.tfidf_th = tfidf_th\n",
    "\n",
    "    def fit(self, x: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Fit the data, calculate probabilities according to it\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): data with shape (N,1), each row consists of a mail\n",
    "            y (np.ndarray): data with shape (N,), each row consists of the label of the mail (0 for ham, 1 for spam)\n",
    "        \"\"\"\n",
    "\n",
    "        # columns is list of every words without their counts, counts stores in count_vector\n",
    "        self.count_vector, columns = self.__vectorizer(x)\n",
    "\n",
    "        self.__calculate_class_prior(y)     # calculates P(spam) and P (ham), and adds them into probability_dict\n",
    "        # calculates P(x(i)|spam) and P(x(i)|ham) values, and adds them into probability_dict\n",
    "        self.__calculate_likelihoods(y, columns, 1)\n",
    "        \n",
    "        if self.use_tfidf:\n",
    "            self.__eliminate_less_important(y, columns)\n",
    "\n",
    "    def predict(self, x_predict: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the labels of the mails in x_predict\n",
    "\n",
    "        Args:\n",
    "            x_predict (np.ndarray): Data to be predicted, shape (N,1)\n",
    "\n",
    "        Returns:\n",
    "            y_predict (np.ndarray): Predictions for the mails, shape (N,)\n",
    "        \"\"\"\n",
    "\n",
    "        n = x_predict.shape[0]\n",
    "        y_predict = np.zeros(n)\n",
    "\n",
    "        vector, columns = self.__vectorizer(x_predict)\n",
    "\n",
    "        for i in range(n):\n",
    "            probability_of_spam = 0\n",
    "            probability_of_ham = 0\n",
    "\n",
    "            word_idx = np.arange(columns.shape[0])[vector[i] > 0] # only work on words that the text has\n",
    "            # calculate P(vj | text)\n",
    "            for j in word_idx:\n",
    "                if \"%s|spam\" % columns[j] in self.probability_dict.keys():\n",
    "                    probability_of_spam += vector[i][j] * self.probability_dict[columns[j] + \"|spam\"]\n",
    "                    probability_of_ham += vector[i][j] * self.probability_dict[columns[j] + \"|ham\"]\n",
    "\n",
    "            probability_of_spam += self.probability_dict[\"spam\"]\n",
    "            probability_of_ham += self.probability_dict[\"ham\"]\n",
    "\n",
    "            y_predict[i] = 1 if probability_of_spam > probability_of_ham else 0\n",
    "\n",
    "        return y_predict\n",
    "\n",
    "    def __vectorizer(self, arr: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Creates a matrix which stores the words that emails includes and their counts in each email\n",
    "\n",
    "        :param arr: an numpy array with shape (N,1) for\n",
    "        :return: a CountVectorizer matrix (N, number of different words in emails) and a vector named columns\n",
    "        with shape (number of different words in emails,1) which stores all words appears in mails\n",
    "        \"\"\"\n",
    "\n",
    "        # initializes CountVectorizer item with ngram_mode\n",
    "        vectorizer = CountVectorizer(ngram_range=self.ngram_mode, stop_words=self.stop_words)\n",
    "\n",
    "        # vector that holds all words in emails and their counts for each item\n",
    "        vector = vectorizer.fit_transform(arr)\n",
    "\n",
    "        # convert vector variable to array for usability\n",
    "        count_vector = vector.toarray()\n",
    "\n",
    "        # names of columns\n",
    "        columns = vectorizer.get_feature_names_out()\n",
    "\n",
    "        return count_vector, columns\n",
    "    \n",
    "    def __eliminate_less_important(self, y : np.ndarray, columns : np.ndarray):\n",
    "        tfidf = TfidfTransformer(use_idf = True, smooth_idf = True)\n",
    "        \n",
    "        tfidf_ham = tfidf.fit_transform(self.count_vector[y == 0]).sum(axis = 0).A1\n",
    "        counts = np.sum(self.count_vector[y == 0] > 0, axis=0)\n",
    "        tfidf_ham /= (1 + counts)\n",
    "        \n",
    "        \n",
    "        tfidf_spam = tfidf.fit_transform(self.count_vector[y == 1]).sum(axis = 0).A1\n",
    "        counts = np.sum(self.count_vector[y == 1] > 0, axis=0)\n",
    "        \n",
    "        tfidf_spam /= (1 + counts)\n",
    "        \n",
    "        idx_ham = np.arange(tfidf_ham.shape[0])[tfidf_ham > self.tfidf_th]\n",
    "        idx_spam = np.arange(tfidf_spam.shape[0])[tfidf_spam > self.tfidf_th]\n",
    "        \n",
    "        old_total = tfidf_ham.shape[0] + tfidf_spam.shape[0]\n",
    "        current_total = idx_ham.shape[0] + idx_spam.shape[0]\n",
    "        print(\"%d words discarded because they fall below the tfidf threshold, predicting with %d word\"\\\n",
    "              % ((old_total - current_total), current_total))\n",
    "        \n",
    "        for doc_class, idx in [(\"ham\", idx_ham), (\"spam\", idx_spam)]:\n",
    "            for i in idx:\n",
    "                self.probability_dict[\"%s|%s\" % (columns[i], doc_class)] = 0\n",
    "        \n",
    "        \n",
    "\n",
    "    def __calculate_class_prior(self, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Calculates class probabilities [P(spam) and P(ham)] for training examples, and adds the result into\n",
    "        self.probability dictionary as \"spam\" and \"ham\" labels\n",
    "\n",
    "        :param y: data with shape (N,), each row consists of the label of the mail (0 for ham, 1 for spam)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        # labels are only 0 and 1 therefore if we sum all items we get number of 1s\n",
    "        # instead of a for loop we can use this method\n",
    "        number_of_spam = np.sum(y)\n",
    "        number_of_ham = len(y) - number_of_spam\n",
    "\n",
    "        self.probability_dict[\"spam\"] = np.log(number_of_spam / y.shape[0])  # P(spam) = number of spams / N\n",
    "        self.probability_dict[\"ham\"] = np.log(number_of_ham / y.shape[0])     # P(ham) = number of hams / N\n",
    "\n",
    "    def __calculate_likelihoods(self, y: np.ndarray, columns: np.ndarray, alpha: int) -> None:\n",
    "        \"\"\"\n",
    "        Calculates likelihoods of each word that contains in emails as P(word|spam) and P(word|ham), and adds the results\n",
    "        into self.probability dictionary as \"word|spam\" and \"word|ham\" labels\n",
    "\n",
    "        :param y: data with shape (N,), each row consists of the label of the mail (0 for ham, 1 for spam)\n",
    "        :param columns: a vector with shape (number of different words in emails,1) which stores all words appears in mails\n",
    "        :param alpha: int value for smoothing\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        N, D = self.count_vector.shape\n",
    "        spam_vector, ham_vector = np.sum(self.count_vector[y == 1], axis=0), np.sum(self.count_vector[y == 0], axis=0)\n",
    "\n",
    "        n_s = np.sum(spam_vector) # | Text_spam |\n",
    "        n_h = np.sum(ham_vector) # | Text_ham  |\n",
    "\n",
    "        for word_i in range(D):\n",
    "            n_w_s = spam_vector[word_i]\n",
    "            n_h_s = ham_vector[word_i]\n",
    "\n",
    "            self.probability_dict[\"%s|spam\" % columns[word_i]] = np.log((n_w_s + alpha) / (n_s + D))\n",
    "            self.probability_dict[\"%s|ham\" % columns[word_i]] = np.log((n_h_s + alpha) / (n_h + D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb1398cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67489 words discarded because they fall below the tfidf threshold, predicting with 107 word\n",
      "Accuracy with tfidf_th = 0.3: 0.9912739965095986\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"emails.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "X, y = df[\"text\"].to_numpy(), df[\"spam\"].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "model = NaiveBayesV2(use_tfidf=True, tfidf_th = 0.3)\n",
    "model.fit(X_train, y_train)\n",
    "y_predict = model.predict(X_test)\n",
    "print(\"Accuracy with tfidf_th = 0.3:\", accuracy_score(y_test.astype(bool), y_predict.astype(bool)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a4203",
   "metadata": {},
   "source": [
    "We tried our model with tfidf threshold value 0.3 and noted important things:\n",
    "1. Total number of used words **decreased by 67489 words**\n",
    "2. Even though only 107 words were used, our accuracy is **0.9912**, which we found quite high.\n",
    "\n",
    "We conclude that using tfidf values helps to find words that are important for certain class, hence discarding unnecessary operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b6ae1d",
   "metadata": {},
   "source": [
    "### 3.b. Stopwords<a class=\"anchor\" id=\"3b\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10f7f4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Highest TFIDF values for class 1 #####\n",
      "['real' 'info' 'buy' 'identity' 'rate' 'remove' 'security' 'contact'\n",
      " 'engines' 'sites' 'internet' 'available' 'look' 'design' 'removed' '2004'\n",
      " 'today' 'creative' 'love' 'mailing' 'oem' 'getting' '19' 'hot' 'wish'\n",
      " 'success' 'received' 'send' 'ready' 'day' 'credit' 'great' 'price' 'help'\n",
      " 'fast' 'work' 'right' 'don' 'good' 'regards' 'address' 'thing' 'prices'\n",
      " 'use' 'know' 'submit' 'hello' 'future' 'stationery' 'web' 'man' 'visit'\n",
      " 'order' '2005' 'interested' 'way' 'net' 'people' 'marketing' 'home'\n",
      " 'offer' 'receive' '10' 'start' 'offers' '000' 'time' 'search' 'account'\n",
      " 'list' 'viagra' 'want' 'life' 'message' 'new' 'www' 'site' 'like' '95'\n",
      " 'mail' 'logo' 'make' 'information' 'best' 'online' 'need' 'save' 'free'\n",
      " 'http' 'just' 'adobe' 'money' 'company' 'email' 'click' 'website'\n",
      " 'software' 'com' 'business' 'subject']\n",
      "##### Highest TFIDF values for class 0 #####\n",
      "['doc' 'office' 'data' 'market' 'options' 'london' 'make' 'send' 'just'\n",
      " 'sent' 'questions' 'schedule' 'credit' 'http' 'shall' 'year' 'kevin'\n",
      " 'development' 'communications' 'summer' 'phone' 'visit' 'project'\n",
      " 'program' 'hi' 'friday' 'good' 'think' '05' 'business' '09' 'university'\n",
      " 'help' 'look' 'best' 'day' 'finance' '08' 'contact' 'mail' 'attached'\n",
      " '03' 'management' '02' 'forwarded' 'john' 'rice' 'presentation' 'week'\n",
      " 'thank' '713' 'work' 'message' 'email' 'regards' 'forward' 'corp'\n",
      " 'resume' 'information' '04' 'crenshaw' 'need' '12' 'power' 'request'\n",
      " 'conference' '11' 'stinson' '01' 'th' 'model' 'houston' 'new' '30' 'edu'\n",
      " 'interview' 'energy' '00' 'like' 'let' '10' 'risk' 'time' 'meeting'\n",
      " 'know' 'group' 'shirley' 'thanks' 'cc' '2001' 'pm' 'research' 'com'\n",
      " 'kaminski' '2000' 'hou' 'subject' 'vince' 'enron' 'ect']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cdd303568e4bde97d484bacf0ba0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating probilities for P(class | word in doc) and P(class | word not in doc):   0%|          | 0/2 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doing presence spam:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doing absence spam:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doing presence ham:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doing absence ham:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "word_p_a_dict_stop = find_presence_absence_probilities(X, y, ENGLISH_STOP_WORDS)\n",
    "\n",
    "take_n = 10 # take n words with highest posterior probility\n",
    "for key, value in word_p_a_dict.items():\n",
    "    word_p_a_dict_stop[key].sort(key= lambda x : x[1])\n",
    "    word_p_a_dict_stop[key] = value[-take_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48c7e408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\tProbability\n",
      "---------------------\n",
      "click\t0.750847\n",
      "search\t0.783951\n",
      "life\t0.827946\n",
      "95\t0.843815\n",
      "save\t0.860793\n",
      "money\t0.882610\n",
      "logo\t0.945333\n",
      "adobe\t1.003532\n",
      "2005\t1.020820\n",
      "viagra\t1.037638\n"
     ]
    }
   ],
   "source": [
    "query = \"spam presence\"\n",
    "print(\"Word\\tProbability\\n---------------------\")\n",
    "for word, prob in word_p_a_dict_stop[query]:\n",
    "    print(\"%s\\t%f\" % (word, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f26bb935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\tProbability\n",
      "---------------------\n",
      "cc\t0.920928\n",
      "hou\t0.921190\n",
      "shirley\t0.921799\n",
      "ect\t0.922348\n",
      "vince\t0.922348\n",
      "enron\t0.922417\n",
      "kaminski\t0.922542\n",
      "crenshaw\t0.923175\n",
      "713\t0.923189\n",
      "stinson\t0.923226\n"
     ]
    }
   ],
   "source": [
    "query = \"ham presence\"\n",
    "print(\"Word\\tProbability\\n---------------------\")\n",
    "for word, prob in word_p_a_dict_stop[query]:\n",
    "    print(\"%s\\t%f\" % (word, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e716920",
   "metadata": {},
   "source": [
    "### 3.c. Analyzing effect of the stopwords<a class=\"anchor\" id=\"3c\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcadd498",
   "metadata": {},
   "source": [
    "Even though we removed stopwords from the corpus we got the same 10 words that suggests a certain class in case of presence. We think that, this happened because of how our pipeline works. Earlier, at the elimination step by using tfidf values, we saw some stopwords in the matrix, but there were none in the output. This happened because when we are calculating their posterior probabilities, we are diving by $P(w)$ which gets higher as word seen frequently in the documents, thus resulting in low $P(w)$. At the end, even though we removed stop words, we got the same words for our results.\n",
    "\n",
    "1. Why might it make sense to remove stop words when interpreting the model?\n",
    "    1. Removing stop words helps to lower the corpus by a certain amount, thus making the model faster. Furthermore since stop words are used too often, it results in high conditional probility even though they are near to meaningless when left alone.\n",
    "2. Why might it make sense to keep stop words?\n",
    "    1. In our opinion, keeping stop words in lower ngram settings doesn't introduce a meaningful increase in prediction. On the other hand, if we were to keep it in higher ngram settings we think it will introduce meaningful combinations may referring to certain idioms, hence it may introduce certain word combinations that has very high conditional probility and it's presence suggesting that the mail belongs to certain class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f18de918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with ngram=unigram, stop_words=False:\t0.989529\n",
      "Accuracy with ngram=unigram, stop_words=True:\t0.990401\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"emails.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "X, y = df[\"text\"].to_numpy(), df[\"spam\"].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=29, stratify=y\n",
    ")\n",
    "\n",
    "args = [(\"unigram\", None), (\"unigram\", ENGLISH_STOP_WORDS)]\n",
    "for arg in args:\n",
    "    model = NaiveBayes(*arg)        # initializes the model\n",
    "    model.fit(X_train, y_train)      # training\n",
    "    y_predict = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test.astype(bool), y_predict.astype(bool))\n",
    "    print(\"Accuracy with ngram=%s, stop_words=%s:\\t%f\" % (arg[0], arg[1] != None, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1004c7",
   "metadata": {},
   "source": [
    "As expected, we see an increase in accuracy when we discard stop words from our model. Our accuracy increased from 0.989529 to 0.990401. We think that, the extra noise coming from calculating conditional probilities of stop words caused this difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53513068",
   "metadata": {},
   "source": [
    "## Part 4 Calculation of Performance Metrics<a class=\"anchor\" id=\"part4\"></a>\n",
    "\n",
    "Below we calculate the wanted performance metrics for different settings of the model.\n",
    "\n",
    "$$\\textbf{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "$$\\textbf{Precision} = \\frac{TP}{TP + FP}$$\n",
    "$$\\textbf{Recall} = \\frac{TP}{TP + FN}$$\n",
    "$$\\textbf{F1 Score} = \\frac{2 * (Precision * Recall)}{Precision + Recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23bcdb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance metrics with settings ngram=unigram, stop_words=True:\n",
      "---------------------------\n",
      "Accuracy:\t0.990401\n",
      "Precision:\t0.978182\n",
      "Recall:\t0.981752\n",
      "F1 Score:\t0.979964\n",
      "\n",
      "\n",
      "Performance metrics with settings ngram=bigram, stop_words=True:\n",
      "---------------------------\n",
      "Accuracy:\t0.990401\n",
      "Precision:\t0.992509\n",
      "Recall:\t0.967153\n",
      "F1 Score:\t0.979667\n",
      "\n",
      "\n",
      "Performance metrics with settings ngram=unigram, stop_words=False:\n",
      "---------------------------\n",
      "Accuracy:\t0.989529\n",
      "Precision:\t0.974638\n",
      "Recall:\t0.981752\n",
      "F1 Score:\t0.978182\n",
      "\n",
      "\n",
      "Performance metrics with settings ngram=bigram, stop_words=False:\n",
      "---------------------------\n",
      "Accuracy:\t0.990401\n",
      "Precision:\t0.996226\n",
      "Recall:\t0.963504\n",
      "F1 Score:\t0.979592\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"emails.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "X, y = df[\"text\"].to_numpy(), df[\"spam\"].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=29, stratify=y\n",
    ")\n",
    "\n",
    "args = [(\"unigram\", ENGLISH_STOP_WORDS), (\"bigram\", ENGLISH_STOP_WORDS), (\"unigram\", None), (\"bigram\", None)]\n",
    "for arg in args:\n",
    "    model = NaiveBayes(*arg)        # initializes the model\n",
    "    model.fit(X_train, y_train)      # training\n",
    "    y_predict = model.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "    \n",
    "    acc = (tp + tn) / (tn + fp + fn + tp)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * recall * precision / (recall + precision)\n",
    "    print(\"Performance metrics with settings ngram=%s, stop_words=%s:\" % (arg[0], arg[1] != None))\n",
    "    print(\"---------------------------\")\n",
    "    print(\"Accuracy:\\t%f\" % acc)\n",
    "    print(\"Precision:\\t%f\" % precision)\n",
    "    print(\"Recall:\\t%f\" % recall)\n",
    "    print(\"F1 Score:\\t%f\" % f1)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
