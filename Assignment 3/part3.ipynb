{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abac30e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "MODES = {\n",
    "    \"unigram\" : (1, 1),\n",
    "    \"bigram\" : (2, 2),\n",
    "    \"uni-bigram\" : (1, 2)\n",
    "}\n",
    "\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, mode: str = \"unigram\", stop_words = None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize NaiveBayes model\n",
    "\n",
    "        Args:\n",
    "            mode (str, optional): Mode for BagOfWords, should be either Unigram or Bigram. Defaults to \"unigram\".\n",
    "            stop_words (list, optional): Stop words to eliminate from BagOfWords\n",
    "        \"\"\"\n",
    "        self.count_vector = None\n",
    "        self.probability_dict = dict()\n",
    "\n",
    "        assert mode in MODES.keys(), \"Mode should be either bigram or unigram\"\n",
    "        self.ngram_mode = MODES[mode]\n",
    "        self.stop_words = stop_words\n",
    "\n",
    "    def fit(self, x: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Fit the data, calculate probabilities according to it\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): data with shape (N,1), each row consists of a mail\n",
    "            y (np.ndarray): data with shape (N,), each row consists of the label of the mail (0 for ham, 1 for spam)\n",
    "        \"\"\"\n",
    "\n",
    "        # columns is list of every words without their counts, counts stores in count_vector\n",
    "        self.count_vector, columns = self.__vectorizer(x)\n",
    "\n",
    "        self.__calculate_class_prior(y)     # calculates P(spam) and P (ham), and adds them into probability_dict\n",
    "        # calculates P(x(i)|spam) and P(x(i)|ham) values, and adds them into probability_dict\n",
    "        self.__calculate_likelihoods(y, columns, 1)\n",
    "\n",
    "    def predict(self, x_predict: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the labels of the mails in x_predict\n",
    "\n",
    "        Args:\n",
    "            x_predict (np.ndarray): Data to be predicted, shape (N,1)\n",
    "\n",
    "        Returns:\n",
    "            y_predict (np.ndarray): Predictions for the mails, shape (N,)\n",
    "        \"\"\"\n",
    "\n",
    "        n = x_predict.shape[0]\n",
    "        y_predict = np.zeros(n)\n",
    "\n",
    "        vector, columns = self.__vectorizer(x_predict)\n",
    "\n",
    "        for i in range(n):\n",
    "            probability_of_spam = 0\n",
    "            probability_of_ham = 0\n",
    "\n",
    "            word_idx = np.arange(columns.shape[0])[vector[i] > 0] # only work on words that the text has\n",
    "            # calculate P(vj | text)\n",
    "            for j in word_idx:\n",
    "                if \"%s|spam\" % columns[j] in self.probability_dict.keys():\n",
    "                    probability_of_spam += vector[i][j] * self.probability_dict[columns[j] + \"|spam\"]\n",
    "                    probability_of_ham += vector[i][j] * self.probability_dict[columns[j] + \"|ham\"]\n",
    "\n",
    "            probability_of_spam += self.probability_dict[\"spam\"]\n",
    "            probability_of_ham += self.probability_dict[\"ham\"]\n",
    "\n",
    "            y_predict[i] = 1 if probability_of_spam > probability_of_ham else 0\n",
    "\n",
    "        return y_predict\n",
    "\n",
    "    def __vectorizer(self, arr: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Creates a matrix which stores the words that emails includes and their counts in each email\n",
    "\n",
    "        :param arr: an numpy array with shape (N,1) for\n",
    "        :return: a CountVectorizer matrix (N, number of different words in emails) and a vector named columns\n",
    "        with shape (number of different words in emails,1) which stores all words appears in mails\n",
    "        \"\"\"\n",
    "\n",
    "        # initializes CountVectorizer item with ngram_mode\n",
    "        vectorizer = CountVectorizer(ngram_range=self.ngram_mode, stop_words=self.stop_words)\n",
    "\n",
    "        # vector that holds all words in emails and their counts for each item\n",
    "        vector = vectorizer.fit_transform(arr)\n",
    "\n",
    "        # convert vector variable to array for usability\n",
    "        count_vector = vector.toarray()\n",
    "\n",
    "        # names of columns\n",
    "        columns = vectorizer.get_feature_names_out()\n",
    "\n",
    "        return count_vector, columns\n",
    "\n",
    "    def __calculate_class_prior(self, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Calculates class probabilities [P(spam) and P(ham)] for training examples, and adds the result into\n",
    "        self.probability dictionary as \"spam\" and \"ham\" labels\n",
    "\n",
    "        :param y: data with shape (N,), each row consists of the label of the mail (0 for ham, 1 for spam)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        # labels are only 0 and 1 therefore if we sum all items we get number of 1s\n",
    "        # instead of a for loop we can use this method\n",
    "        number_of_spam = np.sum(y)\n",
    "        number_of_ham = len(y) - number_of_spam\n",
    "\n",
    "        self.probability_dict[\"spam\"] = np.log(number_of_spam / y.shape[0])  # P(spam) = number of spams / N\n",
    "        self.probability_dict[\"ham\"] = np.log(number_of_ham / y.shape[0])     # P(ham) = number of hams / N\n",
    "\n",
    "    def __calculate_likelihoods(self, y: np.ndarray, columns: np.ndarray, alpha: int) -> None:\n",
    "        \"\"\"\n",
    "        Calculates likelihoods of each word that contains in emails as P(word|spam) and P(word|ham), and adds the results\n",
    "        into self.probability dictionary as \"word|spam\" and \"word|ham\" labels\n",
    "\n",
    "        :param y: data with shape (N,), each row consists of the label of the mail (0 for ham, 1 for spam)\n",
    "        :param columns: a vector with shape (number of different words in emails,1) which stores all words appears in mails\n",
    "        :param alpha: int value for smoothing\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        N, D = self.count_vector.shape\n",
    "        spam_vector, ham_vector = np.sum(self.count_vector[y == 1], axis=0), np.sum(self.count_vector[y == 0], axis=0)\n",
    "\n",
    "        n_s = np.sum(spam_vector) # | Text_spam |\n",
    "        n_h = np.sum(ham_vector) # | Text_ham  |\n",
    "\n",
    "        for word_i in range(D):\n",
    "            n_w_s = spam_vector[word_i]\n",
    "            n_h_s = ham_vector[word_i]\n",
    "\n",
    "            self.probability_dict[\"%s|spam\" % columns[word_i]] = np.log((n_w_s + alpha) / (n_s + D))\n",
    "            self.probability_dict[\"%s|ham\" % columns[word_i]] = np.log((n_h_s + alpha) / (n_h + D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e00570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"emails.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "X, y = df[\"text\"].to_numpy(), df[\"spam\"].to_numpy()\n",
    "\n",
    "model = NaiveBayes()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78999cea",
   "metadata": {},
   "source": [
    "## 3. Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3f2010",
   "metadata": {},
   "source": [
    "### 3.a. Analyzing effect of the words on prediction\n",
    "\n",
    "To analyze the effects of the words on prediction we created a pipeline that works as follows:\n",
    "1. We first eliminate the words in the word set according to their tfidf values for each document class (spam, ham). This step is composed of few steps.\n",
    "    1. Compute tfidf values by using `TfidfTransformer`, we got a matrix of shape (N, D) (where N is equal to sample size and D equal to distinct word count). However we want an output that acts as a scoring metric among words so that we can eliminate among them.\n",
    "    2. Take sum among `axis = 0`, sum by column, this way our matrix is reduced to shape (D,).\n",
    "    3. Take the maximum 100 and minimum 100 tfidf valued words. These 200 words will the candidates for finding the words that suggests certain class by their presence/absence. To be accurate, we used minimum 100 tfidf values to be absence candidates and maximum 100 words to be presence candidates.\n",
    "2. At this step, we see that our saved matrices **contains some stop words**.  To analyze the effects of presence and absence of a word, we compute their posterior probability as follows:\n",
    "$$P(v_d|w) = \\frac{P(w|v_d) * P(v_d)}{P(w)} \\text{ where $P(v_d|w)$ is the probability that the class of the document is $v_d$ given that it contains $w$}$$\n",
    "<center>and\n",
    "$$P(v_d|\\neg w) = \\frac{P(w|v_d) * P(v_d)}{P(\\neg w)} \\text{ where $P(v_d|\\neg w)$ is the probability that the class of the document is $v_d$ given that it doesn't contains $w$}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f24e2",
   "metadata": {},
   "source": [
    "From our $\\text{N$\\dot{a}$ive Bayes}$ model, we know the conditional probilities $P(w|v_d)$, $P(\\neg w|v_d) = 1 - P(w|v_d)$ and prior probilities $P(\\text{spam})$, $P(\\text{ham})$. Hence, we only need to calculate $P(w)$ and $P(\\neg w)$. To calculate those we count the total occurance and divide it by total word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b12060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "\n",
    "def find_presence_absence_probilities (X : np.ndarray, y : np.ndarray, stop_words = None):\n",
    "    ## Create pipelines for two different document class\n",
    "    pipe_spam = Pipeline([('count', CountVectorizer(stop_words = stop_words)),\n",
    "                               ('tfidf', TfidfTransformer(use_idf = True, smooth_idf = True))]).fit(X[y==1])\n",
    "    pipe_ham = Pipeline([('count', CountVectorizer(stop_words = stop_words)),\n",
    "                               ('tfidf', TfidfTransformer(use_idf = True, smooth_idf = True))]).fit(X[y==0])\n",
    "    ## Step 1. Eliminate words that has low tfidf values\n",
    "\n",
    "    y_vals = [(1, pipe_spam), (0, pipe_ham)]\n",
    "    y_words = dict()\n",
    "    for y_val, pipe in y_vals:\n",
    "        tfidf_vals = pipe.transform(X[y==y_val])\n",
    "        words = pipe[\"count\"].get_feature_names_out()\n",
    "        sums = np.sum(tfidf_vals, axis=0).A1\n",
    "        idx = np.argsort(sums)\n",
    "\n",
    "        print(\"##### Highest TFIDF values for class %s #####\" % y_val)\n",
    "        print(words[idx[-100:]])\n",
    "        y_words[\"y = %d\" % y_val] = (idx[:100], idx[-100:])\n",
    "    \n",
    "    ## Step 2. Eliminate words that has low tfidf values\n",
    "    vectorizer = CountVectorizer()\n",
    "    counts = vectorizer.fit_transform(X)\n",
    "    presence_counts = counts.sum(axis=0).A1\n",
    "    total = counts.sum()\n",
    "\n",
    "    doc_classes = [(\"spam\", pipe_spam),\n",
    "                   (\"ham\", pipe_ham)]\n",
    "    word_p_a_dict = dict()\n",
    "    N = X.shape[0]\n",
    "    for doc_class, pipe in tqdm(doc_classes, desc=\"Calculating probilities for P(class | word in doc) and P(class | word not in doc)\"):\n",
    "        word_p_a_dict[\"%s absence\" % doc_class] = []\n",
    "        word_p_a_dict[\"%s presence\" % doc_class] = []\n",
    "        label = 1 if doc_class == \"spam\" else 0\n",
    "\n",
    "        low_tfidf, high_tfidf = y_words[\"y = %d\" % label]\n",
    "        class_prior = model.probability_dict[doc_class]\n",
    "        words = pipe[\"count\"].get_feature_names_out()\n",
    "\n",
    "        for word_idx in tqdm(high_tfidf, leave = False, desc=\"Doing presence %s\" % doc_class):\n",
    "            word = words[word_idx]\n",
    "            word_idx_count = np.where(vectorizer.get_feature_names_out() == word)[0][0]\n",
    "            cond_prob = model.probability_dict[\"%s|%s\" % (word, doc_class)]\n",
    "            presence_count = presence_counts[word_idx_count]\n",
    "            p1 = (cond_prob + class_prior) - np.log(presence_count / total)\n",
    "            word_p_a_dict[\"%s presence\" % doc_class].append((words[word_idx], np.exp(p1)))\n",
    "\n",
    "        for word_idx in tqdm(low_tfidf, leave = False, desc=\"Doing absence %s\" % doc_class):\n",
    "            word = words[word_idx]\n",
    "            word_idx_count = np.where(vectorizer.get_feature_names_out() == word)[0][0]\n",
    "\n",
    "            cond_prob = np.log(1 - np.exp(model.probability_dict[\"%s|%s\" % (word, doc_class)]))\n",
    "            presence_count = presence_counts[word_idx_count]\n",
    "            p1 = cond_prob + class_prior - np.log((total-presence_count) / total)\n",
    "            word_p_a_dict[\"%s absence\" % doc_class].append((words[word_idx], np.exp(p1)))\n",
    "    \n",
    "    return word_p_a_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb031af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Highest TFIDF values for class 1 #####\n",
      "['visit' 'interested' 'how' 'marketing' 'way' 'may' 'was' 'home' 'net'\n",
      " 'about' 'start' '2005' 'receive' 'offer' '10' 'offers' '000' 'time' 'has'\n",
      " 'search' 'only' 'account' 'want' 'but' 'list' 'life' 'what' 'into'\n",
      " 'viagra' 'see' 'new' 'us' 'any' 'like' 'over' 'www' 'site' 'message'\n",
      " 'out' 'make' 'logo' '95' 'information' 'mail' 'one' 'online' 'best'\n",
      " 'please' 'need' 'more' 'save' 'free' 'my' 'no' 'do' 'money' 'now' 'an'\n",
      " 'just' 'company' 'http' 'get' 'email' 'can' 'if' 'all' 'adobe' 'at' 'as'\n",
      " 'click' 'by' 'website' 'business' 'software' 'com' 'here' 'on' 'will'\n",
      " 'or' 'not' 'have' 'are' 'that' 'with' 'from' 'subject' 'be' 'our' 'it'\n",
      " 'we' 'this' 'for' 'is' 'in' 'of' 'your' 'and' 'you' 'the' 'to']\n",
      "##### Highest TFIDF values for class 0 #####\n",
      "['regards' 'him' 'his' 'email' 'forward' '713' 'but' 'up' 'call' 'resume'\n",
      " 'corp' 'some' 'information' 'need' 'get' 'crenshaw' '04' 'so' 'power'\n",
      " '12' 'very' 'conference' 'request' 'stinson' 'th' '11' 'model' 'do'\n",
      " 'houston' '01' 'new' 'all' '30' 'has' 'may' 'was' 'interview' 'about'\n",
      " 'energy' 'like' 'edu' 'let' '00' 'any' 'risk' '10' 'time' 'meeting' 'an'\n",
      " 'know' 'group' 'he' 'shirley' 'not' 'our' 'thanks' 'cc' 'can' 'or' '2001'\n",
      " 'pm' 'by' 'research' 'my' 're' 'if' 'are' 'would' 'as' 'from' 'me'\n",
      " 'please' 'com' 'it' 'am' 'kaminski' '2000' 'at' 'have' 'hou' 'your'\n",
      " 'will' 'with' 'that' 'this' 'be' 'we' 'subject' 'is' 'on' 'vince' 'in'\n",
      " 'for' 'enron' 'of' 'you' 'ect' 'and' 'to' 'the']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1926c30b7cc94997909fa2ebf7b7cfaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating probilities for P(class | word in doc) and P(class | word not in doc):   0%|          | 0/2 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doing presence spam:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doing absence spam:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doing presence ham:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doing absence ham:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_p_a_dict = find_presence_absence_probilities(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd6afd",
   "metadata": {},
   "source": [
    "From the outputs of above cell, we note that highest tfidf values contains some of the stop words such as `is`, `on`, `be`, `of` etc. In below cells, we calculate the posterior probilities similar to how we did in our $\\text{N$\\dot{a}$ive Bayes}$ model. However, since we saved our prior probilities and conditional probilities in log form we continue to do our calculations in log form. Below we show the formulation:\n",
    "$$ln{P(v_d|w)} = ln{\\frac{P(w|v_d) * P(v_d)}{P(w)}} \\tag{1}$$\n",
    "$$ln{P(v_d|w)} = ln{P(w|v_d)} + ln{P(v_d)} - ln{P(w)} \\tag{2}$$\n",
    "<br>\n",
    "<center>and\n",
    "<br>\n",
    "$$ln{P(v_d|\\neg w)} = ln{\\frac{P(\\neg w|v_d) * P(v_d)}{P(\\neg w)}} \\tag{3}$$\n",
    "$$ln{P(v_d|\\neg w)} = ln{P(\\neg w|v_d)} + ln{P(v_d)} - ln{P(\\neg w)} \\tag{4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9766de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "take_n = 10 # take n words with highest posterior probility\n",
    "for key, value in word_p_a_dict.items():\n",
    "    word_p_a_dict[key].sort(key= lambda x : x[1])\n",
    "    word_p_a_dict[key] = value[-take_n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895edbe0",
   "metadata": {},
   "source": [
    "#### 3.a.1. List the 10 words whose presence most strongly predicts that the mail is ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94891f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\tProbability\n",
      "---------------------\n",
      "cc\t0.920928\n",
      "hou\t0.921190\n",
      "shirley\t0.921799\n",
      "ect\t0.922348\n",
      "vince\t0.922348\n",
      "enron\t0.922417\n",
      "kaminski\t0.922542\n",
      "crenshaw\t0.923175\n",
      "713\t0.923189\n",
      "stinson\t0.923226\n"
     ]
    }
   ],
   "source": [
    "query = \"ham presence\"\n",
    "print(\"Word\\tProbability\\n---------------------\")\n",
    "for word, prob in word_p_a_dict[query]:\n",
    "    print(\"%s\\t%f\" % (word, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119a7dc",
   "metadata": {},
   "source": [
    "#### 3.a.2. List the 10 words whose absence most strongly predicts that the mail is ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4680fe13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\tProbability\n",
      "---------------------\n",
      "501\t0.761173\n",
      "unstable\t0.761173\n",
      "discharge\t0.761173\n",
      "interference\t0.761174\n",
      "renovation\t0.761174\n",
      "persistence\t0.761174\n",
      "cloak\t0.761174\n",
      "suffering\t0.761174\n",
      "secrecy\t0.761176\n",
      "php\t0.761206\n"
     ]
    }
   ],
   "source": [
    "query = \"ham absence\"\n",
    "print(\"Word\\tProbability\\n---------------------\")\n",
    "for word, prob in word_p_a_dict[query]:\n",
    "    print(\"%s\\t%f\" % (word, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b329d89d",
   "metadata": {},
   "source": [
    "#### 3.a.3. List the 10 words whose presence most strongly predicts that the mail is spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b60f6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\tProbability\n",
      "---------------------\n",
      "click\t0.750847\n",
      "search\t0.783951\n",
      "life\t0.827946\n",
      "95\t0.843815\n",
      "save\t0.860793\n",
      "money\t0.882610\n",
      "logo\t0.945333\n",
      "adobe\t1.003532\n",
      "2005\t1.020820\n",
      "viagra\t1.037638\n"
     ]
    }
   ],
   "source": [
    "query = \"spam presence\"\n",
    "print(\"Word\\tProbability\\n---------------------\")\n",
    "for word, prob in word_p_a_dict[query]:\n",
    "    print(\"%s\\t%f\" % (word, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2168b",
   "metadata": {},
   "source": [
    "#### 3.a.4. List the 10 words whose absence most strongly predicts that the mail is spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecc55394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\tProbability\n",
      "---------------------\n",
      "respected\t0.238828\n",
      "trash\t0.238829\n",
      "paige\t0.238829\n",
      "determining\t0.238829\n",
      "recovery\t0.238831\n",
      "sets\t0.238833\n",
      "sheet\t0.238835\n",
      "participating\t0.238837\n",
      "sometime\t0.238841\n",
      "steven\t0.238868\n"
     ]
    }
   ],
   "source": [
    "query = \"spam absence\"\n",
    "print(\"Word\\tProbability\\n---------------------\")\n",
    "for word, prob in word_p_a_dict[query]:\n",
    "    print(\"%s\\t%f\" % (word, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f36af9",
   "metadata": {},
   "source": [
    "We see that usually presence of certain words nearly gurantees that the document belongs to certain class. Such as `viagra`, where it's presence strongly suggests that the mail is spam. Similarly, absence of some words suggests to some level that it belongs to certain class. `secrecy` can be a great example for this, where it's absence strongly suggests that the mail is ham.<br>\n",
    "<br>\n",
    "Below we reimplemented our $\\text{N$\\dot{a}$ive Bayes}$ model to use tfidf values to discard not important words. Our implementation discards not important words based on their normalized tfidf values. Where we calculated the normalized tfidf values as follows:\n",
    "1. Sum by axis 0 to get a matrix of shape (C,), where C is the distinct word count\n",
    "2. Divide each column sum by document frequency (df), where df is defined as follows:\n",
    "    $$ df(D, w) = |\\{d \\in D, w \\in d\\}| \\text{, number of documents where word w appears} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4da70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "MODES = {\n",
    "    \"unigram\" : (1, 1),\n",
    "    \"bigram\" : (2, 2),\n",
    "    \"uni-bigram\" : (1, 2)\n",
    "}\n",
    "\n",
    "\n",
    "class NaiveBayesV2:\n",
    "    def __init__(self, mode: str = \"unigram\", stop_words = None, use_tfidf : bool = False, tfidf_th : float = None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize NaiveBayes model\n",
    "\n",
    "        Args:\n",
    "            mode (str, optional): Mode for BagOfWords, should be either Unigram or Bigram. Defaults to \"unigram\".\n",
    "            stop_words (list, optional): Stop words to eliminate from BagOfWords\n",
    "            use_idf (bool, optional)\n",
    "            tfidf_th (float, optional): Threshold for tfidf values, necessary only if use_tfidf is True\n",
    "        \"\"\"\n",
    "        self.count_vector = None\n",
    "        self.probability_dict = dict()\n",
    "\n",
    "        assert mode in MODES.keys(), \"Mode should be either bigram or unigram\"\n",
    "        self.ngram_mode = MODES[mode]\n",
    "        self.stop_words = stop_words\n",
    "        self.use_tfidf = use_tfidf\n",
    "        if use_tfidf:\n",
    "            self.tfidf_th = tfidf_th\n",
    "\n",
    "    def fit(self, x: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Fit the data, calculate probabilities according to it\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): data with shape (N,1), each row consists of a mail\n",
    "            y (np.ndarray): data with shape (N,), each row consists of the label of the mail (0 for ham, 1 for spam)\n",
    "        \"\"\"\n",
    "\n",
    "        # columns is list of every words without their counts, counts stores in count_vector\n",
    "        self.count_vector, columns = self.__vectorizer(x)\n",
    "\n",
    "        self.__calculate_class_prior(y)     # calculates P(spam) and P (ham), and adds them into probability_dict\n",
    "        # calculates P(x(i)|spam) and P(x(i)|ham) values, and adds them into probability_dict\n",
    "        self.__calculate_likelihoods(y, columns, 1)\n",
    "        \n",
    "        if self.use_tfidf:\n",
    "            self.__eliminate_less_important(y, columns)\n",
    "\n",
    "    def predict(self, x_predict: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the labels of the mails in x_predict\n",
    "\n",
    "        Args:\n",
    "            x_predict (np.ndarray): Data to be predicted, shape (N,1)\n",
    "\n",
    "        Returns:\n",
    "            y_predict (np.ndarray): Predictions for the mails, shape (N,)\n",
    "        \"\"\"\n",
    "\n",
    "        n = x_predict.shape[0]\n",
    "        y_predict = np.zeros(n)\n",
    "\n",
    "        vector, columns = self.__vectorizer(x_predict)\n",
    "\n",
    "        for i in range(n):\n",
    "            probability_of_spam = 0\n",
    "            probability_of_ham = 0\n",
    "\n",
    "            word_idx = np.arange(columns.shape[0])[vector[i] > 0] # only work on words that the text has\n",
    "            # calculate P(vj | text)\n",
    "            for j in word_idx:\n",
    "                if \"%s|spam\" % columns[j] in self.probability_dict.keys():\n",
    "                    probability_of_spam += vector[i][j] * self.probability_dict[columns[j] + \"|spam\"]\n",
    "                    probability_of_ham += vector[i][j] * self.probability_dict[columns[j] + \"|ham\"]\n",
    "\n",
    "            probability_of_spam += self.probability_dict[\"spam\"]\n",
    "            probability_of_ham += self.probability_dict[\"ham\"]\n",
    "\n",
    "            y_predict[i] = 1 if probability_of_spam > probability_of_ham else 0\n",
    "\n",
    "        return y_predict\n",
    "\n",
    "    def __vectorizer(self, arr: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Creates a matrix which stores the words that emails includes and their counts in each email\n",
    "\n",
    "        :param arr: an numpy array with shape (N,1) for\n",
    "        :return: a CountVectorizer matrix (N, number of different words in emails) and a vector named columns\n",
    "        with shape (number of different words in emails,1) which stores all words appears in mails\n",
    "        \"\"\"\n",
    "\n",
    "        # initializes CountVectorizer item with ngram_mode\n",
    "        vectorizer = CountVectorizer(ngram_range=self.ngram_mode, stop_words=self.stop_words)\n",
    "\n",
    "        # vector that holds all words in emails and their counts for each item\n",
    "        vector = vectorizer.fit_transform(arr)\n",
    "\n",
    "        # convert vector variable to array for usability\n",
    "        count_vector = vector.toarray()\n",
    "\n",
    "        # names of columns\n",
    "        columns = vectorizer.get_feature_names_out()\n",
    "\n",
    "        return count_vector, columns\n",
    "    \n",
    "    def __eliminate_less_important(self, y : np.ndarray, columns : np.ndarray):\n",
    "        tfidf = TfidfTransformer(use_idf = True, smooth_idf = True)\n",
    "        \n",
    "        tfidf_ham = tfidf.fit_transform(self.count_vector[y == 0]).sum(axis = 0).A1\n",
    "        counts = np.sum(self.count_vector[y == 0] > 0, axis=0)\n",
    "        tfidf_ham /= (1 + counts)\n",
    "        \n",
    "        \n",
    "        tfidf_spam = tfidf.fit_transform(self.count_vector[y == 1]).sum(axis = 0).A1\n",
    "        counts = np.sum(self.count_vector[y == 1] > 0, axis=0)\n",
    "        \n",
    "        tfidf_spam /= (1 + counts)\n",
    "        \n",
    "        idx_ham = np.arange(tfidf_ham.shape[0])[tfidf_ham > self.tfidf_th]\n",
    "        idx_spam = np.arange(tfidf_spam.shape[0])[tfidf_spam > self.tfidf_th]\n",
    "        \n",
    "        old_total = tfidf_ham.shape[0] + tfidf_spam.shape[0]\n",
    "        current_total = idx_ham.shape[0] + idx_spam.shape[0]\n",
    "        print(\"%d words discarded because they fall below the tfidf threshold, predicting with %d word\"\\\n",
    "              % ((old_total - current_total), current_total))\n",
    "        \n",
    "        for doc_class, idx in [(\"ham\", idx_ham), (\"spam\", idx_spam)]:\n",
    "            for i in idx:\n",
    "                self.probability_dict[\"%s|%s\" % (columns[i], doc_class)] = 0\n",
    "        \n",
    "        \n",
    "\n",
    "    def __calculate_class_prior(self, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Calculates class probabilities [P(spam) and P(ham)] for training examples, and adds the result into\n",
    "        self.probability dictionary as \"spam\" and \"ham\" labels\n",
    "\n",
    "        :param y: data with shape (N,), each row consists of the label of the mail (0 for ham, 1 for spam)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        # labels are only 0 and 1 therefore if we sum all items we get number of 1s\n",
    "        # instead of a for loop we can use this method\n",
    "        number_of_spam = np.sum(y)\n",
    "        number_of_ham = len(y) - number_of_spam\n",
    "\n",
    "        self.probability_dict[\"spam\"] = np.log(number_of_spam / y.shape[0])  # P(spam) = number of spams / N\n",
    "        self.probability_dict[\"ham\"] = np.log(number_of_ham / y.shape[0])     # P(ham) = number of hams / N\n",
    "\n",
    "    def __calculate_likelihoods(self, y: np.ndarray, columns: np.ndarray, alpha: int) -> None:\n",
    "        \"\"\"\n",
    "        Calculates likelihoods of each word that contains in emails as P(word|spam) and P(word|ham), and adds the results\n",
    "        into self.probability dictionary as \"word|spam\" and \"word|ham\" labels\n",
    "\n",
    "        :param y: data with shape (N,), each row consists of the label of the mail (0 for ham, 1 for spam)\n",
    "        :param columns: a vector with shape (number of different words in emails,1) which stores all words appears in mails\n",
    "        :param alpha: int value for smoothing\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        N, D = self.count_vector.shape\n",
    "        spam_vector, ham_vector = np.sum(self.count_vector[y == 1], axis=0), np.sum(self.count_vector[y == 0], axis=0)\n",
    "\n",
    "        n_s = np.sum(spam_vector) # | Text_spam |\n",
    "        n_h = np.sum(ham_vector) # | Text_ham  |\n",
    "\n",
    "        for word_i in range(D):\n",
    "            n_w_s = spam_vector[word_i]\n",
    "            n_h_s = ham_vector[word_i]\n",
    "\n",
    "            self.probability_dict[\"%s|spam\" % columns[word_i]] = np.log((n_w_s + alpha) / (n_s + D))\n",
    "            self.probability_dict[\"%s|ham\" % columns[word_i]] = np.log((n_h_s + alpha) / (n_h + D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb1398cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67489 words discarded because they fall below the tfidf threshold, predicting with 107 word\n",
      "Accuracy with tfidf_th = 0.3: 0.9912739965095986\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"emails.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "X, y = df[\"text\"].to_numpy(), df[\"spam\"].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "model = NaiveBayesV2(use_tfidf=True, tfidf_th = 0.3)\n",
    "model.fit(X_train, y_train)\n",
    "y_predict = model.predict(X_test)\n",
    "print(\"Accuracy with tfidf_th = 0.3:\", accuracy_score(y_test.astype(bool), y_predict.astype(bool)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a4203",
   "metadata": {},
   "source": [
    "We tried our model with tfidf threshold value 0.3 and noted important things:\n",
    "1. Total number of used words **decreased by 67489 words**\n",
    "2. Even though only 107 words were used, our accuracy is **0.9912**, which we found quite high.\n",
    "\n",
    "We conclude that using tfidf values helps to find words that are important for certain class, hence discarding unnecessary operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b6ae1d",
   "metadata": {},
   "source": [
    "### 3.b. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10f7f4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Highest TFIDF values for class 1 #####\n",
      "['real' 'info' 'buy' 'identity' 'rate' 'remove' 'security' 'contact'\n",
      " 'engines' 'sites' 'internet' 'available' 'look' 'design' 'removed' '2004'\n",
      " 'today' 'creative' 'love' 'mailing' 'oem' 'getting' '19' 'hot' 'wish'\n",
      " 'success' 'received' 'send' 'ready' 'day' 'credit' 'great' 'price' 'help'\n",
      " 'fast' 'work' 'right' 'don' 'good' 'regards' 'address' 'thing' 'prices'\n",
      " 'use' 'know' 'submit' 'hello' 'future' 'stationery' 'web' 'man' 'visit'\n",
      " 'order' '2005' 'interested' 'way' 'net' 'people' 'marketing' 'home'\n",
      " 'offer' 'receive' '10' 'start' 'offers' '000' 'time' 'search' 'account'\n",
      " 'list' 'viagra' 'want' 'life' 'message' 'new' 'www' 'site' 'like' '95'\n",
      " 'mail' 'logo' 'make' 'information' 'best' 'online' 'need' 'save' 'free'\n",
      " 'http' 'just' 'adobe' 'money' 'company' 'email' 'click' 'website'\n",
      " 'software' 'com' 'business' 'subject']\n",
      "##### Highest TFIDF values for class 0 #####\n",
      "['doc' 'office' 'data' 'market' 'options' 'london' 'make' 'send' 'just'\n",
      " 'sent' 'questions' 'schedule' 'credit' 'http' 'shall' 'year' 'kevin'\n",
      " 'development' 'communications' 'summer' 'phone' 'visit' 'project'\n",
      " 'program' 'hi' 'friday' 'good' 'think' '05' 'business' '09' 'university'\n",
      " 'help' 'look' 'best' 'day' 'finance' '08' 'contact' 'mail' 'attached'\n",
      " '03' 'management' '02' 'forwarded' 'john' 'rice' 'presentation' 'week'\n",
      " 'thank' '713' 'work' 'message' 'email' 'regards' 'forward' 'corp'\n",
      " 'resume' 'information' '04' 'crenshaw' 'need' '12' 'power' 'request'\n",
      " 'conference' '11' 'stinson' '01' 'th' 'model' 'houston' 'new' '30' 'edu'\n",
      " 'interview' 'energy' '00' 'like' 'let' '10' 'risk' 'time' 'meeting'\n",
      " 'know' 'group' 'shirley' 'thanks' 'cc' '2001' 'pm' 'research' 'com'\n",
      " 'kaminski' '2000' 'hou' 'subject' 'vince' 'enron' 'ect']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cdd303568e4bde97d484bacf0ba0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating probilities for P(class | word in doc) and P(class | word not in doc):   0%|          | 0/2 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doing presence spam:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doing absence spam:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doing presence ham:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doing absence ham:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "word_p_a_dict_stop = find_presence_absence_probilities(X, y, ENGLISH_STOP_WORDS)\n",
    "\n",
    "take_n = 10 # take n words with highest posterior probility\n",
    "for key, value in word_p_a_dict.items():\n",
    "    word_p_a_dict_stop[key].sort(key= lambda x : x[1])\n",
    "    word_p_a_dict_stop[key] = value[-take_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48c7e408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\tProbability\n",
      "---------------------\n",
      "click\t0.750847\n",
      "search\t0.783951\n",
      "life\t0.827946\n",
      "95\t0.843815\n",
      "save\t0.860793\n",
      "money\t0.882610\n",
      "logo\t0.945333\n",
      "adobe\t1.003532\n",
      "2005\t1.020820\n",
      "viagra\t1.037638\n"
     ]
    }
   ],
   "source": [
    "query = \"spam presence\"\n",
    "print(\"Word\\tProbability\\n---------------------\")\n",
    "for word, prob in word_p_a_dict_stop[query]:\n",
    "    print(\"%s\\t%f\" % (word, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f26bb935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\tProbability\n",
      "---------------------\n",
      "cc\t0.920928\n",
      "hou\t0.921190\n",
      "shirley\t0.921799\n",
      "ect\t0.922348\n",
      "vince\t0.922348\n",
      "enron\t0.922417\n",
      "kaminski\t0.922542\n",
      "crenshaw\t0.923175\n",
      "713\t0.923189\n",
      "stinson\t0.923226\n"
     ]
    }
   ],
   "source": [
    "query = \"ham presence\"\n",
    "print(\"Word\\tProbability\\n---------------------\")\n",
    "for word, prob in word_p_a_dict_stop[query]:\n",
    "    print(\"%s\\t%f\" % (word, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e716920",
   "metadata": {},
   "source": [
    "### 3.c. Analyzing effect of the stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcadd498",
   "metadata": {},
   "source": [
    "Even though we removed stopwords from the corpus we got the same 10 words that suggests a certain class in case of presence. We think that, this happened because of how our pipeline works. Earlier, at the elimination step by using tfidf values, we saw some stopwords in the matrix, but there were none in the output. This happened because when we are calculating their posterior probabilities, we are diving by $P(w)$ which gets higher as word seen frequently in the documents, thus resulting in low $P(w)$. At the end, even though we removed stop words, we got the same words for our results.\n",
    "\n",
    "1. Why might it make sense to remove stop words when interpreting the model?\n",
    "    1. Removing stop words helps to lower the corpus by a certain amount, thus making the model faster. Furthermore since stop words are used too often, it results in high conditional probility even though they are near to meaningless when left alone.\n",
    "2. Why might it make sense to keep stop words?\n",
    "    1. In our opinion, keeping stop words in lower ngram settings doesn't introduce a meaningful increase in prediction. On the other hand, if we were to keep it in higher ngram settings we think it will introduce meaningful combinations may referring to certain idioms, hence it may introduce certain word combinations that has very high conditional probility and it's presence suggesting that the mail belongs to certain class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f18de918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with ngram=unigram, stop_words=False:\t0.989529\n",
      "Accuracy with ngram=unigram, stop_words=True:\t0.990401\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"emails.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "X, y = df[\"text\"].to_numpy(), df[\"spam\"].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=29, stratify=y\n",
    ")\n",
    "\n",
    "args = [(\"unigram\", None), (\"unigram\", ENGLISH_STOP_WORDS)]\n",
    "for arg in args:\n",
    "    model = NaiveBayes(*arg)        # initializes the model\n",
    "    model.fit(X_train, y_train)      # training\n",
    "    y_predict = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test.astype(bool), y_predict.astype(bool))\n",
    "    print(\"Accuracy with ngram=%s, stop_words=%s:\\t%f\" % (arg[0], arg[1] != None, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1004c7",
   "metadata": {},
   "source": [
    "As expected, we see an increase in accuracy when we discard stop words from our model. Our accuracy increased from 0.989529 to 0.990401. We think that, the extra noise coming from calculating conditional probilities of stop words caused this difference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
